{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5fd9ea-f536-4b8a-8442-a7a929551739",
   "metadata": {},
   "source": [
    "## Import architectures and necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cde28f-e7b1-46af-bc1c-e3106583fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:57:57.046889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-05 15:57:57.604337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from googlenet import GoogLeNet\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b9c83-173f-4ba8-bb60-bbf6144db4a1",
   "metadata": {},
   "source": [
    "## Preprocessing images for train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92cd58f-8589-4160-9077-e22802c31f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "images = []\n",
    "\n",
    "imagesfiles = [i for i in glob('data/*.JPEG')]\n",
    "for i in imagesfiles:\n",
    "    image = cv2.imread(i)/255.0\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    images.append(image)\n",
    "images = np.array(images)\n",
    "\n",
    "with open('data/imagenet_2012_validation_synset_labels.txt') as f:\n",
    "    labelstxt = f.read().split('\\n')[:-1]\n",
    "with open('data/labels.txt') as f:\n",
    "    labs = f.read()\n",
    "    \n",
    "for i in labelstxt:\n",
    "    bosh = labs.find(i)\n",
    "    labels.append(int(labs[bosh+10:bosh+10+labs[bosh+10:].find(' ')])-1)\n",
    "    \n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba3e5db-71ec-447b-bdac-246906ee59cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 224, 224, 3) (40000,)\n",
      "(10000, 224, 224, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=100)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73504a-4187-4345-a29a-214c970b393f",
   "metadata": {},
   "source": [
    "## Generation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea05ba0-56e8-48c4-b00c-46a58c6de2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b31c326-cb96-4103-8dd2-d57afb1417ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(x_train, y_train, 32)\n",
    "test_gen = DataGenerator(x_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77a0b1-76fb-4981-bf31-414a28322994",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd52dc9f-a4d4-439f-8a23-684132711182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 16:02:00.832428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 16:02:00.832646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 16:02:00.857459: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "class Model(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.base_model = GoogLeNet()\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return x\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbffc48-64f4-41d5-81c6-2701d3b67ea8",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9242bd7b-ac16-411e-a505-cd89e89dcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3339e918-d4c4-46b7-843b-6a25cbca9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558b464-24b5-42a0-8524-b4691b8c2445",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f479d859-d579-4d8e-b937-dcb4169a23f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airi/anaconda3/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.907754898071289, Accuracy: 0.09664062410593033%, Test Loss: 6.907761096954346, Test Accuracy: 0.13323158025741577%\n",
      "Epoch 1, Loss: 6.907763481140137, Accuracy: 0.09921875596046448%, Test Loss: 6.907761096954346, Test Accuracy: 0.13323158025741577%\n",
      "Epoch 2, Loss: 6.907752513885498, Accuracy: 0.09781250357627869%, Test Loss: 6.907761096954346, Test Accuracy: 0.13323158025741577%\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for train_images, train_labels in train_gen:\n",
    "        train_step(train_images, train_labels)\n",
    "\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_step(test_images, test_labels)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Loss: {train_loss.result()}, '\n",
    "          f'Accuracy: {train_accuracy.result() * 100}%, '\n",
    "          f'Test Loss: {test_loss.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy.result() * 100}%'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30288dd9-5151-4b3f-952c-fdfeb1907007",
   "metadata": {},
   "source": [
    "## Finetune architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2944cc61-ffd4-48ff-a4d0-4f9788fc9d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 16:15:07.433850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [4]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-05 16:15:07.434123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "2023-05-05 16:15:09.011953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-05-05 16:15:09.012234: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3680, 224, 224, 3),\n",
       " (3680,),\n",
       " (3680, 1),\n",
       " (3669, 224, 224, 3),\n",
       " (3669,),\n",
       " (3669, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = tfds.load('oxford_iiit_pet', split='train')\n",
    "test = tfds.load('oxford_iiit_pet', split='test')\n",
    "\n",
    "x_train, y_train, y_train2, x_test, y_test, y_test2 = [], [], [], [], [], []\n",
    "for i in train:\n",
    "    a = i['image'].numpy()\n",
    "    x_train.append(cv2.resize(a, (224, 224))/255.0)\n",
    "    y_train.append(i['label'].numpy())\n",
    "    y_train2.append(i['species'].numpy())\n",
    "    \n",
    "for i in test:\n",
    "    a = i['image'].numpy()\n",
    "    x_test.append(cv2.resize(a, (224, 224))/255.0)\n",
    "    y_test.append(i['label'].numpy())\n",
    "    y_test2.append(i['species'].numpy())\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train2 = np.array(y_train2)\n",
    "y_train2 = np.expand_dims(y_train2, axis=1)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "y_test2 = np.array(y_test2)\n",
    "y_test2 = np.expand_dims(y_test2, axis=1)\n",
    "\n",
    "x_train.shape, y_train.shape, y_train2.shape, x_test.shape, y_test.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea1cab32-a763-4aba-8e37-6651fd02d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification for two classes\n",
    "class Model1(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.base_model = GoogLeNet(include_top=False)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(1000, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(256, activation='relu')\n",
    "        self.out = keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# classification for 37 classes\n",
    "class Model2(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.base_model = GoogLeNet(include_top=False)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(1000, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(256, activation='relu')\n",
    "        self.out = keras.layers.Dense(37, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "model1 = Model1()\n",
    "model2 = Model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f89e0a0-ec93-4442-bd4d-4d3f6923c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss_object1 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss1 = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy1 = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss1 = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy1 = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "# optimize functions\n",
    "@tf.function\n",
    "def train_step1(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model1(images, training=True)\n",
    "        loss = loss_object1(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model1.trainable_variables)\n",
    "    optimizer1.apply_gradients(zip(gradients, model1.trainable_variables))\n",
    "\n",
    "    train_loss1(loss)\n",
    "    train_accuracy1(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step1(images, labels):\n",
    "    predictions = model1(images, training=False)\n",
    "    t_loss = loss_object1(labels, predictions)\n",
    "    \n",
    "    test_loss1(t_loss)\n",
    "    test_accuracy1(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03928d85-0957-4bc5-9dbf-340e284c3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss_object2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss2 = tf.keras.metrics.Mean(name='train_loss2')\n",
    "train_accuracy2 = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy2')\n",
    "\n",
    "test_loss2 = tf.keras.metrics.Mean(name='test_loss2')\n",
    "test_accuracy2 = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy2')\n",
    "\n",
    "# optimize functions\n",
    "@tf.function\n",
    "def train_step2(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model2(images, training=True)\n",
    "        loss = loss_object2(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model2.trainable_variables)\n",
    "    optimizer2.apply_gradients(zip(gradients, model2.trainable_variables))\n",
    "\n",
    "    train_loss2(loss)\n",
    "    train_accuracy2(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step2(images, labels):\n",
    "    predictions = model2(images, training=False)\n",
    "    t_loss = loss_object2(labels, predictions)\n",
    "    \n",
    "    test_loss2(t_loss)\n",
    "    test_accuracy2(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e791dc-646a-468f-8c4f-9eb35f06f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2 classes\n",
    "train_gen1 = DataGenerator(x_train, y_train2, 32)\n",
    "test_gen1 = DataGenerator(x_test, y_test2, 32)\n",
    "\n",
    "# for 37 classes\n",
    "train_gen2 = DataGenerator(x_train, y_train, 32)\n",
    "test_gen2 = DataGenerator(x_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1916d2-ab83-4bcf-b30c-e70088e55092",
   "metadata": {},
   "source": [
    "### Train 2 classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f1e72a-083f-47c8-8033-5a5ad2c1b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airi/anaconda3/lib/python3.9/site-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6286675333976746, Accuracy: 67.71739196777344%, Test Loss: 0.6137086153030396, Test Accuracy: 67.75688171386719%\n",
      "Epoch 1, Loss: 0.6155099868774414, Accuracy: 67.71739196777344%, Test Loss: 0.5993548631668091, Test Accuracy: 67.75688171386719%\n",
      "Epoch 2, Loss: 0.6036019921302795, Accuracy: 68.1793441772461%, Test Loss: 0.5879561305046082, Test Accuracy: 67.78413391113281%\n",
      "Epoch 3, Loss: 0.5902999043464661, Accuracy: 68.36956787109375%, Test Loss: 0.571384608745575, Test Accuracy: 68.08394622802734%\n",
      "Epoch 4, Loss: 0.5744602084159851, Accuracy: 70.24456024169922%, Test Loss: 0.5462783575057983, Test Accuracy: 72.39029693603516%\n",
      "Epoch 5, Loss: 0.5638549327850342, Accuracy: 70.46195983886719%, Test Loss: 0.5339961051940918, Test Accuracy: 72.58108520507812%\n",
      "Epoch 6, Loss: 0.5542502999305725, Accuracy: 71.41304779052734%, Test Loss: 0.5258346199989319, Test Accuracy: 73.18070220947266%\n",
      "Epoch 7, Loss: 0.5459890365600586, Accuracy: 72.06521606445312%, Test Loss: 0.5209320783615112, Test Accuracy: 73.39874267578125%\n",
      "Epoch 8, Loss: 0.5388891696929932, Accuracy: 72.47283172607422%, Test Loss: 0.5184582471847534, Test Accuracy: 73.53502655029297%\n",
      "Epoch 9, Loss: 0.5322579145431519, Accuracy: 72.82608795166016%, Test Loss: 0.5191696882247925, Test Accuracy: 73.18070220947266%\n",
      "Epoch 10, Loss: 0.5253079533576965, Accuracy: 73.36956024169922%, Test Loss: 0.5250449776649475, Test Accuracy: 73.48051452636719%\n",
      "Epoch 11, Loss: 0.5189543962478638, Accuracy: 73.72282409667969%, Test Loss: 0.5341670513153076, Test Accuracy: 73.15345001220703%\n",
      "Epoch 12, Loss: 0.5141350030899048, Accuracy: 74.51087188720703%, Test Loss: 0.5467480421066284, Test Accuracy: 72.74462127685547%\n",
      "Epoch 13, Loss: 0.5084573030471802, Accuracy: 75.10869598388672%, Test Loss: 0.5614235997200012, Test Accuracy: 72.28128051757812%\n",
      "Epoch 14, Loss: 0.5027387142181396, Accuracy: 75.43478393554688%, Test Loss: 0.571593701839447, Test Accuracy: 71.70890808105469%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss1.reset_states()\n",
    "    train_accuracy1.reset_states()\n",
    "    test_loss1.reset_states()\n",
    "    test_accuracy1.reset_states()\n",
    "\n",
    "    for train_images, train_labels in train_gen1:\n",
    "        train_step1(train_images, train_labels)\n",
    "\n",
    "    for test_images, test_labels in test_gen1:\n",
    "        test_step1(test_images, test_labels)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Loss: {train_loss1.result()}, '\n",
    "          f'Accuracy: {train_accuracy1.result() * 100}%, '\n",
    "          f'Test Loss: {test_loss1.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy1.result() * 100}%'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfe243-4b7b-4946-b728-2539d3417d95",
   "metadata": {},
   "source": [
    "### Train 37 classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f78009-6637-4cb6-a9c9-107f860287de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.622025728225708, Accuracy: 2.88043475151062%, Test Loss: 3.6113758087158203, Test Accuracy: 2.6982829570770264%\n",
      "Epoch 1, Loss: 3.61215877532959, Accuracy: 2.4728260040283203%, Test Loss: 3.6106789112091064, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 2, Loss: 3.611769199371338, Accuracy: 2.3913042545318604%, Test Loss: 3.610917568206787, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 3, Loss: 3.611644983291626, Accuracy: 2.0923912525177%, Test Loss: 3.6108920574188232, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 4, Loss: 3.6114909648895264, Accuracy: 1.9293477535247803%, Test Loss: 3.610886812210083, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 5, Loss: 3.611435651779175, Accuracy: 2.0108695030212402%, Test Loss: 3.6108853816986084, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 6, Loss: 3.6113948822021484, Accuracy: 2.1467392444610596%, Test Loss: 3.6108834743499756, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 7, Loss: 3.611361503601074, Accuracy: 2.11956524848938%, Test Loss: 3.6108815670013428, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 8, Loss: 3.6113524436950684, Accuracy: 2.20108699798584%, Test Loss: 3.6108813285827637, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 9, Loss: 3.611342430114746, Accuracy: 2.11956524848938%, Test Loss: 3.6108806133270264, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 10, Loss: 3.6113314628601074, Accuracy: 2.0108695030212402%, Test Loss: 3.6108813285827637, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 11, Loss: 3.611328125, Accuracy: 2.11956524848938%, Test Loss: 3.610882043838501, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 12, Loss: 3.61132550239563, Accuracy: 2.03804349899292%, Test Loss: 3.6108834743499756, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 13, Loss: 3.611314535140991, Accuracy: 1.8206522464752197%, Test Loss: 3.6108829975128174, Test Accuracy: 2.7255382537841797%\n",
      "Epoch 14, Loss: 3.6113083362579346, Accuracy: 2.1467392444610596%, Test Loss: 3.610884428024292, Test Accuracy: 2.7255382537841797%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss2.reset_states()\n",
    "    train_accuracy2.reset_states()\n",
    "    test_loss2.reset_states()\n",
    "    test_accuracy2.reset_states()\n",
    "\n",
    "    for train_images, train_labels in train_gen2:\n",
    "        train_step2(train_images, train_labels)\n",
    "\n",
    "    for test_images, test_labels in test_gen2:\n",
    "        test_step2(test_images, test_labels)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Loss: {train_loss2.result()}, '\n",
    "          f'Accuracy: {train_accuracy2.result() * 100}%, '\n",
    "          f'Test Loss: {test_loss2.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy2.result() * 100}%'\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713aa0be-04e2-444b-9954-01928f4ec7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
